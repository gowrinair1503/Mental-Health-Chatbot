# -*- coding: utf-8 -*-
"""Untitled61.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qhezpky2QxszhlxoHqp_8idjlWuA7OGq
"""

!pip install datasets

from google.colab import drive
drive.mount('/content/drive')

import json
import pandas as pd

# Load the JSON file
with open('/content/Updated_MentalHealthChatbotDataset_with_overthinking_panic.json') as f:
    data = json.load(f)

intents = data['intents']

# Flatten into a DataFrame
rows = []
for intent in intents:
    tag = intent['tag']
    for pattern in intent['patterns']:
        if pattern.strip():
            rows.append({'pattern': pattern, 'tag': tag})

df = pd.DataFrame(rows)
df.head()

from sklearn.preprocessing import LabelEncoder
from transformers import DistilBertTokenizerFast
import torch

# Encode target labels
le = LabelEncoder()
df['label'] = le.fit_transform(df['tag'])

# Initialize tokenizer
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
encodings = tokenizer(list(df['pattern']), truncation=True, padding=True, return_tensors="pt")
labels = torch.tensor(df['label'].tolist())

from torch.utils.data import Dataset

class ChatDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        # Return a single dictionary with input_ids, attention_mask, and labels
        return {
            'input_ids': self.encodings['input_ids'][idx],
            'attention_mask': self.encodings['attention_mask'][idx],
            'labels': self.labels[idx]
        }

    def __len__(self):
        return len(self.labels)

dataset = ChatDataset(encodings, labels)

"""### Model Training"""

from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(le.classes_))

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=8,
    per_device_train_batch_size=8,
    logging_dir='./logs',
    logging_steps=10,
    save_strategy="no"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)

trainer.train()

import json

with open('/content/drive/MyDrive/Cleaned_MentalHealthChatbotDataset.json', 'r') as f:
    data = json.load(f)

# Check for any intents missing the 'responses' key
missing_responses = [
    intent.get('tag', '[No tag]')
    for intent in data['intents']
    if 'responses' not in intent
]

# Print the problematic tags
if missing_responses:
    print("⚠️ These intents are missing the 'responses' key:")
    print(missing_responses)
else:
    print("✅ All intents have a 'responses' key.")

import torch
import random

# Map tag to responses
response_map = {
    intent['tag']: intent['responses'] # Changed 'responses' to 'user_replies'
    for intent in data['intents']
}

def predict(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    outputs = model(**inputs)
    pred_label_id = torch.argmax(outputs.logits, dim=1).item()
    pred_tag = le.inverse_transform([pred_label_id])[0]
    response = random.choice(response_map[pred_tag])
    return pred_tag, response

import torch
import random

def predict(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    outputs = model(**inputs)
    pred_label_id = torch.argmax(outputs.logits, dim=1).item()
    pred_tag = le.inverse_transform([pred_label_id])[0]
    response = random.choice(response_map[pred_tag])
    return pred_tag, response

def predict(text):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)  # Ensure model is on the correct device

    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to same device

    with torch.no_grad():
        outputs = model(**inputs)
    pred_label_id = torch.argmax(outputs.logits, dim=1).item()
    pred_tag = le.inverse_transform([pred_label_id])[0]
    response = random.choice(response_map[pred_tag])
    return pred_tag, response

print("Yara: Hi, I'm here for your mental wellness. Type 'quit' to end the chat.\n")

while True:
    user_input = input("You: ")
    if user_input.lower() in ['quit', 'exit', 'bye']:
        print("Yara:", random.choice(response_map["goodbye"]))
        break
    tag, response = predict(user_input)
    print(f"Yara ({tag}): {response}")

# Save model and tokenizer
model.save_pretrained("mental_health_chatbot_model")
tokenizer.save_pretrained("mental_health_chatbot_model")

import shutil

# Zip the model directory
shutil.make_archive("mental_health_chatbot_model", 'zip', "mental_health_chatbot_model")

import json

label_mapping = {i: label for i, label in enumerate(le.classes_)}
with open("label_encoder.json", "w") as f:
    json.dump(label_mapping, f)